
## Example Sentence:
Let’s use the simple sentence:

```python
"I love AI"
```


We’ll walk through how embeddings and attention play different roles.



## 1.Embeddings — static context

Each token is mapped to a vector from an embedding matrix (pre-trained or learned):


#### "I"     → [0.1, 0.3, 0.5]
#### "love"  → [0.6, 0.2, 0.9]
#### "AI"    → [0.8, 0.4, 0.7]


### 1.1 What do embeddings do?

 - They encode global relationships across the language — like "king" and "queen" being close, or "cat" and "dog" being semantically similar.

 - But they're static: "I" always gets the same vector regardless of context.

**So embeddings tell us what a word means, not how it's used in this sentence.**


<br><br>


## 2.Attention — dynamic context

Now comes attention: the model asks, for each word,
    “which other words should I pay attention to when understanding this word?”

Let’s say we’re computing attention for the word "love":

We create Q, K, V vectors for each word, then compute dot-product attention.



### 2.1 Hypothetical Attention Weights for "love":
| To     | Attention Weight |
| ------ | ---------------- |
| "I"    | 0.3              |
| "love" | 0.1              |
| "AI"   | 0.6              |

This tells us:

 - When processing "love", the model should focus mostly on "AI", then "I", then a bit on "love" itself.



### 2.2 What attention does:

It uses these weights to dynamically combine the V (value) vectors to produce a context-aware representation of "love".

Now, "love" is not just "love", it's:

 - "love in the context of I and AI".


| Feature               | Embeddings                  | Attention                              |
| --------------------- | --------------------------- | -------------------------------------- |
| Role                  | Static word meaning         | Dynamic sentence-specific focus        |
| Same across use?      | Yes (in vanilla embeddings) | No, depends on surrounding words       |
| Knows about sentence? | ❌ No                        | ✅ Yes                                  |
| Learns context?       | ❌ No                        | ✅ Yes — **per word**, **per sentence** |


<br><br>


## Final Analogy

<span style="background-color: #ffff00">Embeddings = “your fixed personality”

<span style="background-color: #ffff00">Attention = “how much someone listens to others in a conversation depending on the topic”

You might be "funny" (embedding), but whether that matters in a conversation depends on context — attention decides how important that trait is right now.






<br><br>




## 3.Typical Embedding Sizes in Practice

In real models, embedding vectors are much longer, often hundreds or even thousands of dimensions.
 
| Model                 | Embedding Size (`d_model`) |
| --------------------- | -------------------------- |
| GPT-2 Small           | 768                        |
| BERT Base             | 768                        |
| GPT-3                 | 2048                       |
| GPT-4 (rumored)       | 8192                       |
| BioGPT (clinical LLM) | 768                        |



These embeddings are vectors like:


```python
# Example: embedding for the token "love"
[0.123, -0.044, ..., 0.881]  # 768 or more dimensions
```


### 3.1 Why So Long?

There are two main reasons:

 - **1.Capture complex information:**
High-dimensional vectors can store nuanced aspects of meaning — grammar, sentiment, subject/object roles, etc.


 - **2.Enable rich transformations:**
Attention mechanisms and feed-forward layers operate in this space, so a larger dimension means more capacity to learn and express complex patterns.



### 3.2 In Small Demos
For learning purposes (like I love AI), we shrink it down to 3–10 dimensions to make it visible and computable by hand. But in real models, yes — it's much bigger.

<br><br>

## 4.Embeddings get updated during training

### During pretraining (e.g., training BERT, GPT from scratch):

 - The embedding matrix is randomly initialized.

 - As the model trains (e.g., to predict next tokens), gradients flow from the loss all the way back to the embeddings.

 - So embeddings are updated just like other weights (using backpropagation and optimizers like Adam).

**The model learns to adjust word vectors so that they work well with the attention and other layers to minimize prediction error.**

<br>

### How does it look?

Say you have an embedding matrix:

```python
embedding_matrix = nn.Embedding(vocab_size, d_model)

#When you train:

loss.backward()
optimizer.step()
```

The embedding matrix's weights get updated like everything else — they’re just like W_q, W_k, etc.

<br>

### 4.1 During fine-tuning or transfer learning:

| Option                 | What happens to embeddings?        |
| ---------------------- | ---------------------------------- |
| **Train everything**   | Embeddings are updated             |
| **Freeze embeddings**  | Embeddings stay fixed (no updates) |
| **Use new embeddings** | You may initialize or fine-tune    |
































