
A Feedforward Neural Network is the simplest type of artificial neural network:

Key Features:
 - Data flows in one direction → from input → to hidden layers → to output.

 - No loops or cycles.

 - Often used in basic tasks like classification or regression.

```python
Input → Hidden Layer → Output

output = activation(weights * input + bias)
```


# Full Breakdown of a Feedforward Neural Network (FFN)


## 1. Definition

A Feedforward Neural Network (FFN) is the most basic type of neural network where:

 - Information flows only forward from the input to the output.

 - There are no cycles or feedback connections.



<br><br>


## 2. Main Components

 - An FFN consists of:

 - Input Layer

 - One or more Hidden Layers

 - Output Layer

 - Weights and Biases

 - Activation Functions



<br><br>


## 3. Input Layer

 - Accepts input features from the dataset.

 - Each neuron represents a feature.

 - Example: For 3 input features [x1, x2, x3], the input layer has 3 neurons.


<br><br>


## 4. Weights and Biases

 - Each connection between neurons has a weight (w) and bias (b).

 - These are the parameters the network learns.

 - **Initially, weights and biases are usually randomly initialized.**

<br><br>

## 5. Linear Transformation

 - Each neuron performs a weighted sum:



```python
z = w₁·x₁ + w₂·x₂ + ... + wₙ·xₙ + b
```
Or in vector notation:

```python
z = W·x + b
```


Where:

 - W = weight matrix

 - x = input vector

 - b = bias vector



<br><br>



## 6. Activation Function
 - Applies a non-linear transformation to the output of each neuron:

```python
a = activation(z)
```

Common activations:

 - ReLU (max(0, x))

 - Sigmoid (for binary outputs)

 - Tanh

 - Softmax (for probabilities in multi-class outputs)



<br><br>




## 7. Hidden Layers
 - Layers between input and output.

 - Each neuron in a hidden layer connects to all neurons in the previous layer.

 - Purpose: To learn complex patterns in the data.




<br><br>




## 8. Output Layer

 - Final predictions of the network.

 - Output depends on the task:

   - Regression: Linear or no activation

   - Binary classification: Sigmoid activation

   - Multi-class classification: Softmax activation


<br><br>




## 9. Forward Propagation
This is the core "feedforward" part:

 - Input passes through layers using:

```python
a = activation(W·x + b)
```


<br><br>


##  10. Loss Function
 - Measures how far predictions are from actual targets.

 - Examples:

   - Mean Squared Error (MSE) → regression

   - Cross-Entropy Loss → classification

<br><br>


##  11. Backpropagation

 - Computes gradients of loss w.r.t. each weight and bias using chain rule of calculus.

 - Efficiently updates all layers from output back to input.

<br><br>

## 12. Optimization (Gradient Descent)

 - After computing gradients, update weights:

```python
w := w - learning_rate × ∂loss/∂w
b := b - learning_rate × ∂loss/∂b
```

 - Optimizers: SGD, Adam, RMSprop, etc.

<br><br>


## 13. Training Loop
 - This process repeats over many epochs:

 - Feed input through the network

 - Compute loss

 - Backpropagate error

 - Update weights

 - Repeat on all batches






# Example in NumPy

```python
import numpy as np

def relu(x): return np.maximum(0, x)

# Input
X = np.array([[0.5, 0.2]])

# Layer 1
W1 = np.array([[0.1, 0.3], [0.2, 0.4]])
b1 = np.array([0.1, 0.1])
Z1 = X @ W1 + b1
A1 = relu(Z1)

# Layer 2
W2 = np.array([[0.6], [0.9]])
b2 = np.array([0.05])
Z2 = A1 @ W2 + b2

print("Output:", Z2)
```




## Advantages
 - Simple and fast to implement

 - Good for structured/tabular data

 - Works well on small to medium-sized problems


## Limitations

 - Cannot model sequences or time-dependent data

 - Requires large data for deeper networks

 - Struggles with spatial or temporal data (e.g., images, audio, text)

## Variants of FFNs
 - Other types of neural networks:

 - Convolutional Neural Networks (CNNs) → image data

 - Recurrent Neural Networks (RNNs) → sequential data

 - Transformers → attention-based, for NLP

 - Autoencoders → unsupervised representation learning
