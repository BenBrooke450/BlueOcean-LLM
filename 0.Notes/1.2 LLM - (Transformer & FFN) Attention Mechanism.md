







# <> 4. Compute Attention Scores

We’ll now calculate how much each word should attend to every other word.

This is done by computing the dot product of Query (Q) and Key (K).


![img_6.png](Images/img_6.png)

 
Where:

 - Q = Query matrix of shape (3 × 2)

 - K = Key matrix of shape (3 × 2) ⇒ so **transpose** is (2 × 3)

Result: (3 × 3) matrix = score each token gives to every token

![img_7.png](Images/img_7.png)

<br>

![img_8.png](Images/img_8.png)

<br>


|             | **K: I** | **K: love** | **K: AI** |
| ----------- | -------- | ----------- | --------- |
| **Q: I**    | 1        | 0           | 1         |
| **Q: love** | 1        | 1           | 2         |
| **Q: AI**   | 2        | 1           | 3         |


<br><br><br>

