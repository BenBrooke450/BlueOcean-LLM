
## Example Sentence:
Let’s use the simple sentence:

```python
"I love AI"
```


We’ll walk through how embeddings and attention play different roles.



## Step 1: Embeddings — static context

Each token is mapped to a vector from an embedding matrix (pre-trained or learned):


### "I"     → [0.1, 0.3, 0.5]
### "love"  → [0.6, 0.2, 0.9]
### "AI"    → [0.8, 0.4, 0.7]


### What do embeddings do?

 - They encode global relationships across the language — like "king" and "queen" being close, or "cat" and "dog" being semantically similar.

 - But they're static: "I" always gets the same vector regardless of context.

**So embeddings tell us what a word means, not how it's used in this sentence.**


<br><br>


## Step 2: Attention — dynamic context

Now comes attention: the model asks, for each word,
    “which other words should I pay attention to when understanding this word?”

Let’s say we’re computing attention for the word "love":

We create Q, K, V vectors for each word, then compute dot-product attention.



### Hypothetical Attention Weights for "love":
| To     | Attention Weight |
| ------ | ---------------- |
| "I"    | 0.3              |
| "love" | 0.1              |
| "AI"   | 0.6              |

This tells us:

 - When processing "love", the model should focus mostly on "AI", then "I", then a bit on "love" itself.

